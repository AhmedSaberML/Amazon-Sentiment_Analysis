{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQxhn1bJhYIe"
      },
      "source": [
        "# **importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "CTstGmztJA-J",
        "outputId": "a624994c-beb5-43b8-8a15-00640c2d233f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "spacy.load('en_core_web_sm')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk.tokenize import  word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "import html\n",
        "import unicodedata\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "# from sklearn.metrics import plot_confusion_matrix\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim import models\n",
        "from gensim.models import Word2Vec, word2vec\n",
        "from gensim.models import FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpedG68Hhdi1"
      },
      "source": [
        "# **Download dataset from kaggle**\n",
        "https://www.kaggle.com/datasets/sid321axn/amazon-alexa-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOFv8lB4vwFV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCjcair_I9mR"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/kaggle_dataset/S_A 1/amazon_alexa.tsv', sep ='\\t')\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Byu5vmxCH9"
      },
      "source": [
        "# **Exploring Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4Amzsm5qykU"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okmay5OovRhe"
      },
      "outputs": [],
      "source": [
        "data['feedback'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6THGg_AsASI"
      },
      "source": [
        "# **Data Formating**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfc1aGwtYmwC"
      },
      "outputs": [],
      "source": [
        "data.drop([\"rating\",\t\"date\"\t,\"variation\"], axis=1, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkxHoINAx0_t"
      },
      "source": [
        "# **Pre-Processing Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIwINilsq_DY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_non_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "def replace_numbers(text):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_whitespaces(text):\n",
        "    return text.strip()\n",
        "\n",
        "def remove_stopwords(words, stop_words):\n",
        "    \"\"\"\n",
        "    :param words:\n",
        "    :type words:\n",
        "    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "    or\n",
        "    from spacy.lang.en.stop_words import STOP_WORDS\n",
        "    :type stop_words:\n",
        "    :return:\n",
        "    :rtype:\n",
        "    \"\"\"\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    \"\"\"Lemmatize words in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
        "\n",
        "def text2words(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "def normalize_text( text):\n",
        "    text = remove_non_ascii(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = to_lowercase(text)\n",
        "    text = replace_numbers(text)\n",
        "    words = text2words(text)\n",
        "    words = remove_stopwords(words, stop_words)\n",
        "    words = lemmatize_words(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "\n",
        "    return ''.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ2F7zO-q-7h"
      },
      "outputs": [],
      "source": [
        "# data['verified_reviews']=data['verified_reviews'].apply text: normalize_text (text))\n",
        "data_clean = data['verified_reviews'].apply(normalize_text)\n",
        "data_clean[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqSsln_UOrWR"
      },
      "source": [
        "# **Final Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97OgOR0RGf-O"
      },
      "outputs": [],
      "source": [
        "data[\"final_text\"]=data_clean\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBE8cfV1yuXF"
      },
      "source": [
        "# **Vectoraization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05UZzkeN2EwE"
      },
      "source": [
        "# **Word2vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1h9CZIzCF3G"
      },
      "outputs": [],
      "source": [
        "num_features = 100 # Word vector dimensionality\n",
        "min_word_count = 40   # Minimum word count\n",
        "num_workers = 3       # Number of threads to run in parallel\n",
        "context = 10          # Context window size\n",
        "downsampling = 1e-3   # Downsample setting for frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyhiyS-lBCse"
      },
      "outputs": [],
      "source": [
        "# # tokenized_tweet = data['final_text'].apply(lambda x: x.split()) # tokenizing\n",
        "\n",
        "# # model_w2v = gensim.models.Word2Vec( tokenized_tweet,workers=num_workers,size=num_features, min_count = min_word_count,window = context, sample = downsampling,sg=1)\n",
        "# # model_w2v.train(tokenized_tweet, total_examples= len( data['final_text']), epochs=20)\n",
        "# model_w2v.wv.most_similar(\"good\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGTbI7droH7P"
      },
      "outputs": [],
      "source": [
        "# def word_vector(tokens, size):\n",
        "#     vec = np.zeros(size).reshape((1, size))\n",
        "#     count = 0\n",
        "#     for word in tokens:\n",
        "#         try:\n",
        "#             vec += model_w2v[word].reshape((1, size))\n",
        "#             count += 1.\n",
        "#         except KeyError:  # handling the case where the token is not in vocabulary\n",
        "#             continue\n",
        "#     if count != 0:\n",
        "#         vec /= count\n",
        "#     return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0d_XpO8BP6m"
      },
      "outputs": [],
      "source": [
        "# wordvec_arrays = np.zeros((len(tokenized_tweet), 400))\n",
        "# for i in range(len(tokenized_tweet)):\n",
        "#     wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 400)\n",
        "# wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "# wordvec_df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNQCF94JpgeQ"
      },
      "outputs": [],
      "source": [
        "# y=data['feedback']\n",
        "# X_train, X_test, y_train, y_test = train_test_split(wordvec_df, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6FoF-ugBZqm"
      },
      "source": [
        "# **Fasttext model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqumbA_2BcSP"
      },
      "outputs": [],
      "source": [
        "tokenized_tweet = data['final_text'].apply(lambda x: x.split()) # tokenizing\n",
        "model_ft = FastText( tokenized_tweet,workers=num_workers,size=num_features, min_count = min_word_count,window = context, sample = downsampling,sg=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWISKRjDCrzN"
      },
      "outputs": [],
      "source": [
        "model_ft.most_similar(positive=\"good\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltRKc1JoC3O2"
      },
      "outputs": [],
      "source": [
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0.\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += model_ft[word].reshape((1, size))\n",
        "            count += 1.\n",
        "        except KeyError: # handling the case where the token is not in vocabulary\n",
        "\n",
        "            continue\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZXEH1tcDE4G"
      },
      "outputs": [],
      "source": [
        "ft_arrays = np.zeros((len(tokenized_tweet), 100))\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    ft_arrays[i,:] = word_vector(tokenized_tweet[i], 100)\n",
        "fasttext_df = pd.DataFrame(ft_arrays)\n",
        "fasttext_df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue8zrso7DdWN"
      },
      "outputs": [],
      "source": [
        "y=data['feedback']\n",
        "X_train, X_test, y_train, y_test = train_test_split(fasttext_df, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2r-xPdDNW45"
      },
      "source": [
        "# **Compare Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R8SwnL_NG96"
      },
      "source": [
        "# **SVC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xquMAwxjnyeS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "svc = LinearSVC()\n",
        "svc.fit(X_train,y_train)\n",
        "y_pred_svc=svc.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(classification_report(y_test,y_pred_svc))\n",
        "# print(accuracy_score(y_test,y_pred_svc))\n",
        "# pd.crosstab(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90250ZAQP7QS"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix(y_test, y_pred_svc), annot=True, fmt='g');\n",
        "plt.title('Confusion Matrix', fontsize=10);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kroR5CXTPgmg"
      },
      "source": [
        "# **L.R**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aFGdUxUPlkg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_lr=logreg.predict(X_test)\n",
        "print(classification_report(y_test,y_pred_lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfQIU1K1M0N5"
      },
      "source": [
        "# **D.T**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYOdfe7JNpnK"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train,y_train)\n",
        "y_pred_dt=dt.predict(X_test)\n",
        "print(classification_report(y_test,y_pred_dt))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4fR1uPzsR-W"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='g');\n",
        "plt.title('Confusion Matrix', fontsize=10);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYGCfHtZM2Y4"
      },
      "source": [
        "# **R.F**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vodWb7pNlUg"
      },
      "outputs": [],
      "source": [
        "#Random Forest - Ensemble of Descision Trees\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators=20)\n",
        "rf.fit(X_train,y_train)\n",
        "y_pred_rf=rf.predict(X_test)\n",
        "print(classification_report(y_test,y_pred_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4kurZbZsMeY"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='g');\n",
        "plt.title('Confusion Matrix', fontsize=10);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMZlWpA4NIPw"
      },
      "source": [
        " **K-NN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEHe-4JJPz6P"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=7)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_knn=knn.predict(X_test)\n",
        "print(classification_report(y_test,y_pred_knn))\n",
        "# print(knn.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwl00_o2sAde"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix(y_test, y_pred_knn), annot=True, fmt='g');\n",
        "plt.title('Confusion Matrix', fontsize=10);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnXgL-H7N-MH"
      },
      "source": [
        "# **Ada Boost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyRzkm4ZOClg"
      },
      "outputs": [],
      "source": [
        "from sklearn. ensemble import AdaBoostClassifier\n",
        "adb = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators = 5, learning_rate = 1)\n",
        "adb.fit(X_train,y_train)\n",
        "y_pred_adb=adb.predict(X_test)\n",
        "print(classification_report(y_test,y_pred_adb))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwI0KlmBrVNP"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix(y_test, y_pred_adb), annot=True, fmt='g');\n",
        "plt.title('Confusion Matrix', fontsize=10);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aayrQilkMVCh"
      },
      "source": [
        "# **xgboost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDoFWg2XOYsA"
      },
      "outputs": [],
      "source": [
        "\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "import xgboost as xgb\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train,y_train)\n",
        "y_pred_xg=xgb.predict(X_test)\n",
        "print(classification_report(y_test,y_pred_xg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCYF5c01rsHV"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix(y_test, y_pred_xg), annot=True, fmt='g');\n",
        "plt.title('Confusion Matrix', fontsize=10);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJefsjZgMYWz"
      },
      "source": [
        "# **Bagging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwDv97DSN5co"
      },
      "outputs": [],
      "source": [
        "from sklearn. ensemble import BaggingClassifier\n",
        "bg = BaggingClassifier(DecisionTreeClassifier(), max_samples= 0.5, max_features = 1.0, n_estimators = 20)\n",
        "bg.fit(X_train,y_train)\n",
        "y_pred_bg=bg.predict(X_test)\n",
        "print(classification_report(y_test,y_pred_bg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKZKaLVgrwOv"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix(y_test, y_pred_bg), annot=True, fmt='g');\n",
        "plt.title('Confusion Matrix', fontsize=10);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAvOiEOKQHo6"
      },
      "source": [
        "# **SMOTE(Handling imblanced data)**\n",
        "# **Best Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1PlEMnAI3qO"
      },
      "outputs": [],
      "source": [
        "# pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HelfGDJmI3nw"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train,y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23g2ieS4I3jO"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "print(\"Before SMOTE :\" , Counter(y_train))\n",
        "print(\"After SMOTE :\" , Counter(y_train_smote))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sBZKGAcI3gt"
      },
      "outputs": [],
      "source": [
        "clf=svc.fit(X_train_smote,y_train_smote)\n",
        "y_predict = clf.predict(X_test)\n",
        "print(classification_report(y_test,y_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPQ9PH15JA8l"
      },
      "outputs": [],
      "source": [
        " sns.heatmap(confusion_matrix(y_test, y_predict), annot=True, fmt='g');\n",
        "plt.title('Confusion Matrix', fontsize=10);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5vDyT-NHgKQ"
      },
      "source": [
        "# **ROC-AUC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoR7TYoCHk-g"
      },
      "outputs": [],
      "source": [
        "svc = LinearSVC()\n",
        "svc.fit(X_train,y_train)\n",
        "y_pred_svm = svc.decision_function(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u-z89-rHk1o"
      },
      "outputs": [],
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_logistic = logreg.decision_function(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ex7S9QKCr4"
      },
      "outputs": [],
      "source": [
        "adb = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators = 5, learning_rate = 1)\n",
        "adb.fit(X_train,y_train)\n",
        "y_pred_adb=adb.decision_function(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ysmXVYALE3P"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_smote,y_train_smote)\n",
        "y_predict = clf.decision_function(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuUC6LCjBWQZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "logistic_fpr, logistic_tpr, threshold = roc_curve(y_test, y_pred_logistic)\n",
        "auc_logistic = auc(logistic_fpr, logistic_tpr)\n",
        "\n",
        "adb_fpr, adb_tpr, threshold = roc_curve(y_test, y_pred_adb)\n",
        "auc_adb = auc(adb_fpr, adb_tpr)\n",
        "\n",
        "svm_fpr, svm_tpr, threshold = roc_curve(y_test, y_pred_svm)\n",
        "auc_svm = auc(svm_fpr, svm_tpr)\n",
        "\n",
        "clf_fpr, clf_tpr, threshold = roc_curve(y_test, y_predict)\n",
        "auc_clf = auc(clf_fpr, clf_tpr)\n",
        "\n",
        "plt.figure(figsize=(5, 5), dpi=100)\n",
        "plt.plot(logistic_fpr, logistic_tpr, marker='.', label='Logistic (auc = %0.3f)' % auc_logistic)\n",
        "plt.plot(svm_fpr, svm_tpr, linestyle='-', label='SVM (auc = %0.3f)' % auc_svm)\n",
        "plt.plot(adb_fpr, adb_tpr, linestyle='-', label='adb (auc = %0.3f)' % auc_adb)\n",
        "plt.plot(clf_fpr, clf_tpr, linestyle='-', label='clf (auc = %0.3f)' % auc_clf)\n",
        "\n",
        "plt.xlabel('False Positive Rate -->')\n",
        "plt.ylabel('True Positive Rate -->')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zuNibkfrS7Q"
      },
      "source": [
        "# **Test Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx2fMtMYok9a"
      },
      "outputs": [],
      "source": [
        "par=model_ft[\"liked \"].reshape(1,-1)\n",
        "clf.predict(par)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6XiLPIBqY3R"
      },
      "source": [
        "# **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL-_NzvRndUb"
      },
      "outputs": [],
      "source": [
        "#  import pickle\n",
        "#  pickle.dump(clf,open('model','wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}